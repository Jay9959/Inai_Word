import os
import re
import ujson as json
import builtins
from collections import defaultdict
from hashlib import blake2b
from tqdm import tqdm
import gc
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import multiprocessing as mp
import threading
from functools import lru_cache

# orjson рк╡рк╛рккрк░рлЛ (ркЬрлЛ рк╣рлЛркп ркдрлЛ) тАУ 3x ркЭркбрккрлА JSON
try:
    import orjson
    # orjson рк╕рлАркзрлЛ ркмрк╛ркЗркирк░рлА ркЖркЙркЯрккрлБркЯ ркЖрккрлЗ ркЫрлЗ, ркдрлЗркерлА .decode() ркирк╣рлАркВ
    json.load = lambda f: orjson.loads(f.read())
    json.dump = lambda obj, f, indent=None: f.buffer.write(
        orjson.dumps(obj, option=orjson.OPT_INDENT_2) if indent else orjson.dumps(obj)
    )
    print("тЬЕ orjson рк╡рк╛рккрк░рлАркирлЗ JSON рк╕рлНрккрлАркб рк╡ркзрк╛рк░рлА.")
except ImportError:
    print("тЪая╕П orjson ркиркерлА, ujson рк╡рк╛рккрк░рлА рк░рк╣рлНркпрк╛ ркЫрлАркП.")

# SpaCy ркЯрлЛркХркирк╛ркЗркЭрк░ (ркЬрлЛ рк╣рлЛркп ркдрлЛ)
try:
    from spacy.lang.en import English
    nlp = English()
    tokenizer = nlp.tokenizer
    print("тЬЕ SpaCy ркЯрлЛркХркирк╛ркЗркЭрк░ рк╡рк╛рккрк░рлА рк░рк╣рлНркпрк╛ ркЫрлАркП.")
except ImportError:
    print("тЪая╕П SpaCy ркиркерлА. рк░рлЗркЧрлЗркХрлНрк╕ рк╡рк╛рккрк░рлА рк░рк╣рлНркпрк╛ ркЫрлАркП. (pip install spacy ркХрк░рлЛ)")
    tokenizer = None

from datasets import load_dataset

# --- ркдркорк╛рк░рк╛ ркорк╛рк░рлНркЧ ркЕркирлЗ рк╕рлЗркЯрк┐ркВркЧрлНрк╕ ---
OUTPUT_FOLDER = r"D:/data/2Connections"
VOCAB_FILE = r"D:\data\output.txt"
GLOBAL_TENSOR_IDS_FILE = os.path.join(OUTPUT_FOLDER, "global_tensor_ids.json")

# тЬЕ ркЕрк╣рлАркВ рклркХрлНркд ркЖ 3 рк▓рк╛ркЗрки ркмркжрк▓рлЛ
# --- ркбрлЗркЯрк╛рк╕рлЗркЯ рк╕рлЗркЯрк┐ркВркЧрлНрк╕ ---
HF_DATASET_NAME = "rakshitdabral/gutenberg-tiny"
HF_DATASET_SPLITS = ["train"]  # ркорлЛркЯрлЗ ркнрк╛ркЧрлЗ 'train' ркЬ рк╣рлЛркп ркЫрлЗ
HF_DATASET_TEXT_COLUMNS = ["text"]  # <--- ркорлБркЦрлНркп ркХрлЙрк▓рко "text" ркЫрлЗ

# рклрк╛ркЗрк▓ ркУрккрк░рлЗрк╢рки ркорк╛ркЯрлЗ рк▓рлЙркХ
file_lock = threading.Lock()

# рк╕рк╛ркЗркирлНркЯрк┐рклрк┐ркХ ркирлЛркЯрлЗрк╢рки ркорк╛ркЯрлЗ рккрлЗркЯрк░рлНрки
sci_notation_pattern = re.compile(r'^-?\d+(\.\d+)?e[+-]?\d+$', re.IGNORECASE)

# ркЧрлНрк▓рлЛркмрк▓ ркЯрлЗркирлНрк╕рк░ ID ркорлЗрккрк┐ркВркЧ
word_to_tensor_id = {}
word_tensor_id_lock = threading.Lock()

# ркмркзрлА рккрлНрк░рлЗркбрк┐ркХрлНрк╢рки рклрк╛ркЗрк▓рлНрк╕ RAM ркорк╛ркВ ркХрлЗрк╢
loaded_predictions_cache = {}
predictions_cache_lock = threading.Lock()

# рккрлНрк░рлА-ркХркорлНрккрк╛ркЗрк▓рлНркб ркЯрлЛркХркирк╛ркЗркЭрлЗрк╢рки рккрлЗркЯрк░рлНрки
TOKEN_REGEX = re.compile(r"\b[a-z']+\b")

# ЁЯФБ ркбрк┐ркЯрк░ркорк┐ркирк┐рк╕рлНркЯрк┐ркХ ркЯрлЗркирлНрк╕рк░ ID
def generate_global_tensor_id(word):
    with word_tensor_id_lock:
        if word in word_to_tensor_id:
            return word_to_tensor_id[word]
        unique_str = word.encode("utf-8")
        h = blake2b(unique_str, digest_size=6).hexdigest()
        int_val = int(h, 16)
        tensor_id = round((int_val % 99999999) / 100000000.0, 8)
        val_str = str(tensor_id)
        if sci_notation_pattern.match(val_str):
            salt = 0
            while True:
                h_salted = blake2b(unique_str + str(salt).encode(), digest_size=6).hexdigest()
                int_val_salted = int(h_salted, 16)
                candidate = round((int_val_salted % 99999999) / 100000000.0, 8)
                if not sci_notation_pattern.match(str(candidate)):
                    tensor_id = candidate
                    break
                salt += 1
        word_to_tensor_id[word] = tensor_id
        return tensor_id

# ЁЯФН рк░рк┐рккрлАркЯрлЗркЯрк┐рк╡ рк╢ркмрлНркжрлЛ рклрк┐рк▓рлНркЯрк░ ркХрк░рлЛ тАУ @lru_cache рк╕рк╛ркерлЗ (ркЭркбрккрлА)
@lru_cache(maxsize=50000)
def is_repetitive(word):
    """ркПркХ ркЬ ркЕркХрлНрк╖рк░ рк╡рк╛рк░ркВрк╡рк╛рк░ рк╣рлЛркп ркдрлЛ рклрк┐рк▓рлНркЯрк░ ркХрк░рлЛ."""
    n = len(word)
    if n > 40: return True
    if n <= 1: return False
    if word == word[0] * n: return True
    if len(set(word)) <= 2 and n > 5 and word.count(word[0]) > n * 0.8: return True
    if any(c in word for c in '<>:"/\\|?*'): return True
    return False

# FIX: Helper function for defaultdict, making it pickleable
def create_nested_defaultdict_int():
    return defaultdict(int)

# ЁЯз╣ ркЯрлЛркХркирк╛ркЗркЭ ркЕркирлЗ ркЬрлЛркбрлАркУ ркмркирк╛рк╡рлЛ
def process_text_entries_batch(entries, vocab):
    local_pairs = defaultdict(create_nested_defaultdict_int)
    for entry in entries:
        try:
            text_parts = []
            for col in HF_DATASET_TEXT_COLUMNS:
                val = entry.get(col)
                if val and isinstance(val, str):
                    text_parts.append(val.lower())
                elif val and isinstance(val, dict) and 'text' in val:
                    text_parts.append(val['text'].lower())
            text_content = " ".join(text_parts)
            if not text_content:
                continue

            if tokenizer:
                words = [token.text for token in tokenizer(text_content) if token.is_alpha or "'" in token.text]
            else:
                words = TOKEN_REGEX.findall(text_content)
            
            for i in range(len(words) - 1):
                w1, w2 = words[i], words[i + 1]
                if w1 in vocab and not is_repetitive(w1) and not is_repetitive(w2):
                    local_pairs[w1][w2] += 1
        except Exception as e:
            continue
    return local_pairs

# ЁЯФЧ ркмрлЗ ркбрк┐ркХрлНрк╢ркирк░рлА ркЬрлЛркбрлЛ
def merge_word_pairs(main, local):
    for w1, nexts in local.items():
        for w2, cnt in nexts.items():
            main[w1][w2] += cnt

# ЁЯУВ ркПркХ рклрк╛ркЗрк▓ рк▓рлЛркб ркХрк░рлЛ
def load_single_prediction_file(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        word = os.path.basename(filepath).replace("_predictions.json", "")
        return (word, data)
    except Exception as e:
        return None

# ЁЯУВ ркмркзрлА рккрлНрк░рлЗркбрк┐ркХрлНрк╢рки рклрк╛ркЗрк▓рлНрк╕ RAM ркорк╛ркВ рк▓рлЛркб ркХрк░рлЛ
def preload_all_predictions(output_folder):
    print("ЁЯФД ркмркзрлА рккрлНрк░рлЗркбрк┐ркХрлНрк╢рки рклрк╛ркЗрк▓рлНрк╕ RAM ркорк╛ркВ рк╕ркорк╛ркВркдрк░ рк▓рлЛркб ркХрк░рлА рк░рк╣рлНркпрк╛ ркЫрлАркП...")
    cache = {}
    file_paths = [os.path.join(output_folder, fname) for fname in os.listdir(output_folder) if fname.endswith("_predictions.json")]
    
    num_threads = min(32, os.cpu_count() * 2)
    
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        results = list(tqdm(executor.map(load_single_prediction_file, file_paths), 
                            total=len(file_paths), 
                            desc="ЁЯУд рклрк╛ркЗрк▓рлЛ рк▓рлЛркб ркХрк░рлА рк░рк╣рлНркпрк╛ ркЫрлАркП", miniters=100))
    
    count = 0
    for result in results:
        if result:
            word, data = result
            cache[word] = data
            count += 1
    print(f"тЬЕ {count} рклрк╛ркЗрк▓рлНрк╕ рк▓рлЛркб ркеркИ.")
    return cache

# ЁЯТ╛ ркПркХ рклрк╛ркЗрк▓ рк╕рлЗрк╡ ркХрк░рлЛ (ркмрк╛ркЗркирк░рлА ркорлЛркбркорк╛ркВ)
def save_one_file(args):
    word, predictions = args
    try:
        filename = f"{word}_predictions.json"
        out_file = os.path.join(OUTPUT_FOLDER, filename)
        with file_lock:
            with open(out_file, 'wb') as f:  # 'wb' тЖТ ркмрк╛ркЗркирк░рлА ркорлЛркб
                json.dump(predictions, f, indent=2)
        return True
    except Exception as e:
        print(f"тЭМ {word} ркорк╛ркЯрлЗ рклрк╛ркЗрк▓ рк╕рлЗрк╡ ркирк╛ ркеркИ: {e}")
        return False

# --- ркорлБркЦрлНркп рккрлНрк░рлЛрк╕рлЗрк╕рк┐ркВркЧ ---
def process_huggingface_dataset_splits():
    global loaded_predictions_cache, word_to_tensor_id

    # global_tensor_ids.json рк▓рлЛркб ркХрк░рлЛ
    print(f"ЁЯФД {GLOBAL_TENSOR_IDS_FILE} ркорк╛ркВркерлА ркЯрлЗркирлНрк╕рк░ ID рк▓рлЛркб ркХрк░рлА рк░рк╣рлНркпрк╛ ркЫрлАркП...")
    try:
        if os.path.exists(GLOBAL_TENSOR_IDS_FILE):
            with open(GLOBAL_TENSOR_IDS_FILE, 'r', encoding='utf-8') as f:
                loaded_ids = json.load(f)
                word_to_tensor_id.update(loaded_ids)
            print(f"тЬЕ {len(word_to_tensor_id)} рк╣рк╛рк▓ркирк╛ ркЯрлЗркирлНрк╕рк░ ID рк▓рлЛркб ркеркпрк╛.")
        else:
            print("тД╣я╕П global_tensor_ids.json ркорк│рлА ркиркерлА, рк╢рк░рлВркЖркдркерлА ркмркирк╛рк╡рлА рк░рк╣рлНркпрк╛ ркЫрлАркП.")
    except Exception as e:
        print(f"тЭМ global_tensor_ids.json рк▓рлЛркб ркХрк░рк╡рк╛ркорк╛ркВ ркнрлВрк▓: {e}")
        word_to_tensor_id.clear()

    # vocab.txt рк▓рлЛркб ркХрк░рлЛ
    print("ЁЯУЪ vocab.txt рк▓рлЛркб ркХрк░рлАркирлЗ ркЯрлЗркирлНрк╕рк░ ID ркЬркирк░рлЗркЯ ркХрк░рлА рк░рк╣рлНркпрк╛ ркЫрлАркП...")
    vocab = set()
    try:
        with open(VOCAB_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                word = line.strip().lower()
                if word:
                    vocab.add(word)
                    generate_global_tensor_id(word)
        print(f"тЬЕ {len(vocab)} рк╢ркмрлНркжрлЛ рк▓рлЛркб ркеркпрк╛.")
    except FileNotFoundError:
        print(f"тЭМ {VOCAB_FILE} ркиркерлА ркорк│ркдрлА.")
        return

    # рккрлНрк░рлЗркбрк┐ркХрлНрк╢рки рклрк╛ркЗрк▓рлНрк╕ рккрлНрк░рлА-рк▓рлЛркб
    loaded_predictions_cache = preload_all_predictions(OUTPUT_FOLDER)

    discovered_splits = HF_DATASET_SPLITS
    print(f"ЁЯУМ ркбрлЗркЯрк╛рк╕рлЗркЯ: {HF_DATASET_NAME}, рк╕рлНрккрлНрк▓рк┐ркЯ: {discovered_splits}")

    num_cpus = mp.cpu_count()
    data_workers = max(1, num_cpus - 1)
    write_workers = min(16, num_cpus * 2)

    for current_split in discovered_splits:
        print(f"\n--- ЁЯЪА рккрлНрк░рлЛрк╕рлЗрк╕рк┐ркВркЧ: {current_split} ---")
        try:
            dataset = load_dataset(HF_DATASET_NAME, split=current_split, streaming=True)
            print("тЬЕ ркбрлЗркЯрк╛рк╕рлЗркЯ рк╕рлНркЯрлНрк░рлАркорк┐ркВркЧ ркорлЛркбркорк╛ркВ рк▓рлЛркб ркеркИ.")
        except Exception as e:
            print(f"тЭМ рк▓рлЛркб ркирк╛ ркеркИ: {e}")
            continue

        word_pair_freq = defaultdict(create_nested_defaultdict_int)
        total_entries = 0
        batch_size = 10000
        buffer = []

        with ProcessPoolExecutor(max_workers=data_workers) as executor:
            futures = []
            for entry in tqdm(dataset, desc="ЁЯФд ркбрлЗркЯрк╛ ркПркХркдрлНрк░рк┐ркд ркХрк░рлА рк░рк╣рлНркпрк╛ ркЫрлАркП", miniters=1000):
                buffer.append(entry)
                if len(buffer) >= batch_size * data_workers:  # chunksize рк╕рлБркзрк╛рк░рлЛ
                    chunked = [buffer[i:i + batch_size] for i in range(0, len(buffer), batch_size)]
                    for chunk in chunked:
                        futures.append(executor.submit(process_text_entries_batch, chunk, vocab))
                    buffer = []

                    # ркнрк╡рк┐рк╖рлНркп рккрлВрк░рлНркг ркерк╛ркп ркдрлНркпрк╛рк░рлЗ ркорк░рлНркЬ ркХрк░рлЛ
                    if len(futures) >= data_workers * 3:
                        completed = as_completed(futures, timeout=None).__next__()
                        local = completed.result()
                        futures.remove(completed)
                        merge_word_pairs(word_pair_freq, local)
                        total_entries += batch_size

            # ркмрк╛ркХрлАркирк╛ ркмрклрк░ ркЕркирлЗ рклрлНркпрлБркЪрк░рлНрк╕
            if buffer:
                futures.append(executor.submit(process_text_entries_batch, buffer, vocab))
            for future in as_completed(futures):
                local = future.result()
                merge_word_pairs(word_pair_freq, local)
                total_entries += batch_size

            # рклркХрлНркд ркЬрк░рлВрк░ рккркбрлЗ ркдрлНркпрк╛рк░рлЗ ркЬ gc
            if total_entries % 50000 == 0:
                gc.collect()

        print(f"ЁЯУК рккрлЗрк░ рклрлНрк░рк┐ркХрлНрк╡ркирлНрк╕рлА рккрлВрк░рлА. ~{total_entries} ркПркирлНркЯрлНрк░рлАркЭ, {len(word_pair_freq)} рк╢ркмрлНркжрлЛ.")

        # рккрлНрк░рлЗркбрк┐ркХрлНрк╢рки ркЕрккркбрлЗркЯ ркХрк░рлЛ
        final_updates = {}
        words = list(word_pair_freq.keys())
        for word in tqdm(words, desc="ЁЯФД рккрлНрк░рлЗркбрк┐ркХрлНрк╢рки ркдрлИркпрк╛рк░ ркХрк░рлА рк░рк╣рлНркпрк╛ ркЫрлАркП"):
            if is_repetitive(word) or len(f"{word}_predictions.json") > 200:
                continue

            predictions = loaded_predictions_cache.get(word, {})
            updated = False

            for next_word in word_pair_freq[word]:
                tid = generate_global_tensor_id(next_word)
                if predictions.get(next_word) != tid:
                    predictions[next_word] = tid
                    updated = True

            if updated:
                sorted_preds = {
                    k: v for k, v in sorted(predictions.items(), key=lambda x: -x[1])
                    if len(k) <= 20
                }
                final_updates[word] = sorted_preds
                loaded_predictions_cache[word] = sorted_preds

        # рклрк╛ркЗрк▓рлНрк╕ рк╕рлЗрк╡ ркХрк░рлЛ
        print(f"ЁЯТ╛ {len(final_updates)} рклрк╛ркЗрк▓рлНрк╕ рк╕рлЗрк╡ ркХрк░рлА рк░рк╣рлНркпрк╛ ркЫрлАркП...")
        with ThreadPoolExecutor(max_workers=write_workers) as writer:
            list(tqdm(
                writer.map(save_one_file, final_updates.items()),
                total=len(final_updates),
                desc="ЁЯУд рклрк╛ркЗрк▓рлНрк╕ рк╕рлЗрк╡ ркХрк░рлА рк░рк╣рлНркпрк╛ ркЫрлАркП"
            ))

        print(f"тЬЕ {current_split} ркорк╛ркЯрлЗ рккрлНрк░рлЛрк╕рлЗрк╕рк┐ркВркЧ рккрлВрк░рлА.")

    # global_tensor_ids.json рк╕рлЗрк╡ ркХрк░рлЛ
    try:
        with open(GLOBAL_TENSOR_IDS_FILE, "w", encoding="utf-8") as f:
            json.dump(word_to_tensor_id, f, indent=2)
        print("тЬЕ global_tensor_ids.json рк╕рклрк│ркдрк╛рккрлВрк░рлНрк╡ркХ ркЕрккркбрлЗркЯ ркеркИ.")
    except Exception as e:
        print(f"тЭМ global_tensor_ids.json рк╕рлЗрк╡ ркХрк░рк╡рк╛ркорк╛ркВ ркнрлВрк▓: {e}")

# --- ркорлБркЦрлНркп рклркВркХрлНрк╢рки ---
if __name__ == "__main__":
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    try:
        process_huggingface_dataset_splits()
    except KeyboardInterrupt:
        print("\nЁЯЫС ркпрлБркЭрк░рлЗ ркмркВркз ркХрк░рлНркпрлБркВ.")
    except Exception as e:
        print(f"\nЁЯТе ркПрк░рк░: {e}")
        import traceback
        traceback.print_exc()